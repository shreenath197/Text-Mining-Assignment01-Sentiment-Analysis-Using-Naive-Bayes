{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b80faeb-1925-49d2-bf1b-3bd106cbcef6",
   "metadata": {},
   "source": [
    "# Assignment-1\n",
    "\n",
    "Cohort 11 - PGP in AI/ML\n",
    "\n",
    "C5 - Text Mining\n",
    "\n",
    "Assignment - Sentiment Analysis Using Naive Bayes \n",
    "Perform Text Classification on the data. The tweets related to coronavirus have been pulled from Twitter, and manual tagging has been done. \n",
    "\n",
    "You might use some of the References given below:  \n",
    "\n",
    "1. Sklearn Pipeline\n",
    "\n",
    "2. Sklearn GridSearchCV\n",
    "\n",
    "3. ML Pipeline with Grid Search in Scikit-Learn \n",
    "\n",
    "\n",
    "\n",
    "Dataset: Coronavirus tweets NLP - Text Classification\n",
    "\n",
    " \n",
    "\n",
    "The steps to be performed are as follows: \n",
    "Read dataset and perform Text processing for the tweets ( Remove Stop words, and special characters and convert the text to lowercase) -  1 Mark\n",
    "Using the train_test_split function of Sklearn, Split the kaggle's train dataset further into train, and test dataset - 1 Mark \n",
    "Use BoW and TF-IDF based feature extraction approaches on the \"text\" field of the dataset. You can use existing library functions. [2+2 marks] \n",
    "Create model building pipeline and define parameters for GridSearch (You might Refer to the code below) - 2 Mark \n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())]) \n",
    " \n",
    "tuned_parameters = {\n",
    "                   'vect_ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "                   'tfidf___use_idf': (True, False),\n",
    "                   'tfidf____norm': ('11', '12'),\n",
    "                   'clf_alpha': [1, 1e-1, 1e-2]\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "5. Perform classification (using GridSearch) - 2 Marks \n",
    "6. Print the confusion matrix, accuracy, and F1 score on the test dataset - 1 Mark  \n",
    "\n",
    "7. Interpret your results in terms of Business Domain Knowledge. 1 Mark \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bb152-baa9-4b95-97bd-348b52a03a75",
   "metadata": {},
   "source": [
    "## Task 1:- Read dataset and perform Text processing for the tweets ( Remove Stop words, and special characters and convert the text to lowercase) -  1 Mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d149f8e-7821-4ca6-8c33-8008c7971082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings as war\n",
    "war.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011a1b08-87b5-44ae-bf5b-c565ce9b1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetPath=r\"C:\\Users\\ASUS\\jupyterworkspace\\Assignment & Mini Project\\Module_05_Text Mining\\Text-Mining-Assignment01-Sentiment-Analysis-Using-Naive-Bayes\\Corona_NLP_train.csv\"\n",
    "dataSetRead=pd.read_csv(dataSetPath,encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdb669a-5fd2-4b34-896f-79a57c83ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************Displaying below first 5 records**********************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying first 5 records to confirming data loading\n",
    "print(\"*****************************************************Displaying below first 5 records**********************************************************\")\n",
    "dataSetRead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2048420b-ea60-443a-b7f5-b4678be5bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of Dataset:- (41157, 6)\n",
      "Total number of rows in Dataset:- 41157\n",
      "Total number of columns in Dataset:- 6\n"
     ]
    }
   ],
   "source": [
    "# Displaying dimension of dataSet\n",
    "print(\"Dimention of Dataset:- {}\".format(dataSetRead.shape[0:2]))\n",
    "print(\"Total number of rows in Dataset:- {}\".format(dataSetRead.shape[0]))\n",
    "print(\"Total number of columns in Dataset:- {}\".format(dataSetRead.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39cead8-0d6c-4f51-a4c5-a86dd418593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevent featured from dataSet\n",
    "dataSetRead=dataSetRead[['OriginalTweet','Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e97301-1c6d-42cf-8e8b-8e787ccfd0ab",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290f9114-7889-42df-a283-cff5b6ff87dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing required packages\n",
    "from nltk.corpus import stopwords # nltk.corpus.stopwords: Provides a collection of common stopwords for multiple languages.\n",
    "from nltk.tokenize import word_tokenize # nltk.tokenize.word_tokenize: A tokenizer that splits text into individual words\n",
    "import nltk # import nltk: The Natural Language Toolkit is a library used for natural language processing tasks.\n",
    "\n",
    "# Downloading the stopwords and punkt tokenizer if not already downloaded\n",
    "nltk.download('stopwords') # nltk.download('stopwords'): Ensures the required stopword dataset is downloaded locally\n",
    "nltk.download('punkt') # nltk.download('punkt'): Downloads the punkt tokenizer model, which is needed for tokenizing text into words or sentences\n",
    "\n",
    "# Getting the list of English stopwords\n",
    "stop_words = set(stopwords.words('english')) # stopwords.words('english'): Retrieves a predefined list of English stopwords\n",
    "\n",
    "# Functioning to remove stopwords\n",
    "def remove_stopwords(text): # def remove_stopwords(text):: Defines a function to clean text by removing stopwords\n",
    "    if not isinstance(text, str): # if not isinstance(text, str):: Checks if the input is a string; if not, it returns the input unchanged\n",
    "        return text  \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Applying the function to the 'OriginalTweet' column\n",
    "dataSetRead['text_cleaned_OriginalTweet'] = dataSetRead['OriginalTweet'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b7ade-7bd5-441a-8238-03389d6e294d",
   "metadata": {},
   "source": [
    "### Removing the special characters and convert the text to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c5a8e49-bec0-4ba7-8338-2dc2a03be010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************Displaying below the first 10 rocords of updated dataframe*********************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text_cleaned_OriginalTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>menyrbie  philgahan  chrisitv https  tcoifzfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk neighbours family exchange phone n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia  woolworths give elderly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>food stock one empty  please  nt panic  enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>ready go supermarket  covid outbreak  m paran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>news regions first confirmed covid case came s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cashier at grocery store was sharing his insig...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>cashier grocery store sharing insights  covid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>supermarket today  nt buy toilet paper   rebel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Due to COVID-19 our retail store and classroom...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>due covid retail store classroom atlanta open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>corona prevention  stop buy things cash use on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>All month there hasn't been crowding in the su...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>month nt crowding supermarkets restaurants  ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Due to the Covid-19 situation, we have increas...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>due covid situation  increased demand food pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#horningsea is a caring community. LetÂs ALL ...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>horningsea caring community  lets look less c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Me: I don't need to stock up on food, I'll jus...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>nt need stock food  ll amazon deliver whateve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADARA Releases COVID-19 Resource Center for Tr...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>adara releases covid resource center travel br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lines at the grocery store have been unpredict...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>lines grocery store unpredictable  eating safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>????? ????? ????? ????? ??\\r\\r\\n?????? ????? ?...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@eyeonthearctic 16MAR20 Russia consumer survei...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>eyeonthearctic mar russia consumer surveillan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Amazon Glitch Stymies Whole Foods, Fresh Groce...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>amazon glitch stymies whole foods  fresh groce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For those who aren't struggling, please consid...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>nt struggling  please consider donating food b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        OriginalTweet           Sentiment  \\\n",
       "0   @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1   advice Talk to your neighbours family to excha...            Positive   \n",
       "2   Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3   My food stock is not the only one which is emp...            Positive   \n",
       "4   Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "5   As news of the regionÂs first confirmed COVID...            Positive   \n",
       "6   Cashier at grocery store was sharing his insig...            Positive   \n",
       "7   Was at the supermarket today. Didn't buy toile...             Neutral   \n",
       "8   Due to COVID-19 our retail store and classroom...            Positive   \n",
       "9   For corona prevention,we should stop to buy th...            Negative   \n",
       "10  All month there hasn't been crowding in the su...             Neutral   \n",
       "11  Due to the Covid-19 situation, we have increas...  Extremely Positive   \n",
       "12  #horningsea is a caring community. LetÂs ALL ...  Extremely Positive   \n",
       "13  Me: I don't need to stock up on food, I'll jus...            Positive   \n",
       "14  ADARA Releases COVID-19 Resource Center for Tr...            Positive   \n",
       "15  Lines at the grocery store have been unpredict...            Positive   \n",
       "16  ????? ????? ????? ????? ??\\r\\r\\n?????? ????? ?...             Neutral   \n",
       "17  @eyeonthearctic 16MAR20 Russia consumer survei...             Neutral   \n",
       "18  Amazon Glitch Stymies Whole Foods, Fresh Groce...  Extremely Positive   \n",
       "19  For those who aren't struggling, please consid...            Positive   \n",
       "\n",
       "                           text_cleaned_OriginalTweet  \n",
       "0    menyrbie  philgahan  chrisitv https  tcoifzfa...  \n",
       "1   advice talk neighbours family exchange phone n...  \n",
       "2   coronavirus australia  woolworths give elderly...  \n",
       "3   food stock one empty  please  nt panic  enough...  \n",
       "4    ready go supermarket  covid outbreak  m paran...  \n",
       "5   news regions first confirmed covid case came s...  \n",
       "6   cashier grocery store sharing insights  covid ...  \n",
       "7   supermarket today  nt buy toilet paper   rebel...  \n",
       "8   due covid retail store classroom atlanta open ...  \n",
       "9   corona prevention  stop buy things cash use on...  \n",
       "10  month nt crowding supermarkets restaurants  ho...  \n",
       "11  due covid situation  increased demand food pro...  \n",
       "12   horningsea caring community  lets look less c...  \n",
       "13   nt need stock food  ll amazon deliver whateve...  \n",
       "14  adara releases covid resource center travel br...  \n",
       "15  lines grocery store unpredictable  eating safe...  \n",
       "16                                                ...  \n",
       "17   eyeonthearctic mar russia consumer surveillan...  \n",
       "18  amazon glitch stymies whole foods  fresh groce...  \n",
       "19  nt struggling  please consider donating food b...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing required package\n",
    "import re\n",
    "\n",
    "# Functioning to preprocess text\n",
    "def preprocess_text(text):\n",
    "# Removing punctuation and special characters using regex\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keeping only words and spaces\n",
    "# Converting text to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Applying the preprocessing function to the 'text' column\n",
    "dataSetRead['text_cleaned_OriginalTweet'] = dataSetRead['text_cleaned_OriginalTweet'].apply(preprocess_text)\n",
    "\n",
    "# Displaying the 10 five rocords of updated dataFrame\n",
    "print(\"***********************************************Displaying below the first 10 rocords of updated dataframe*********************************************\")\n",
    "dataSetRead.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64caaa-58fa-43cf-b4f9-b5a33f68c5b2",
   "metadata": {},
   "source": [
    "## Task 2:- Using the train_test_split function of Sklearn, Split the kaggle's train dataset further into train, and test dataset - 1 Mark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368c5bb5-75a7-4cd3-924f-9af93a37ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 28809\n",
      "X_test shape: 12348\n",
      "y_train shape: 28809\n",
      "y_test shape: 12348\n"
     ]
    }
   ],
   "source": [
    "# Importing required package\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Defining the feature (X) and the target variable (y)\n",
    "X=dataSetRead['OriginalTweet']\n",
    "y=dataSetRead['Sentiment']\n",
    "# Split the dataset into training and testing subsets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "# Displaying the shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape[0])\n",
    "print(\"X_test shape:\", X_test.shape[0])\n",
    "print(\"y_train shape:\", y_train.shape[0])\n",
    "print(\"y_test shape:\", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e3a5e-7940-4269-93b2-a4913bf723d2",
   "metadata": {},
   "source": [
    "### Task 3:- Use BoW and TF-IDF based feature extraction approaches on the \"text\" field of the dataset. You can use existing library functions. [2+2 marks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba961dcd-01a8-4b7f-953a-051c756fc283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW - Training Data Shape: (28809, 62185)\n",
      "BoW - Testing Data Shape: (12348, 62185)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of the transformed matrices\n",
    "print(\"BoW - Training Data Shape:\", X_train_bow.shape)\n",
    "print(\"BoW - Testing Data Shape:\", X_test_bow.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404d03f9-ede8-46ae-88a6-15b85e9a51dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.04%\n"
     ]
    }
   ],
   "source": [
    "sparsity = (X_train_bow.nnz / (X_train_bow.shape[0] * X_train_bow.shape[1])) * 100\n",
    "print(f\"Sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2112ad37-12ff-4c3c-9005-8bd0da2b9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Training Data Shape: (28809, 62185)\n",
      "TF-IDF - Testing Data Shape: (12348, 62185)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of the transformed matrices\n",
    "print(\"TF-IDF - Training Data Shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF - Testing Data Shape:\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96fad58c-adbd-4a69-96fb-589b5815e680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.04%\n"
     ]
    }
   ],
   "source": [
    "sparsity = (X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100\n",
    "print(f\"Sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c00180-281e-473c-a552-ec50b7139346",
   "metadata": {},
   "source": [
    "## Task 4:- Create model building pipeline and define parameters for GridSearch (You might Refer to the code below) - 2 Mark \n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())]) \n",
    " \n",
    "tuned_parameters = {\n",
    "                   'vect_ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "                   'tfidf___use_idf': (True, False),\n",
    "                   'tfidf____norm': ('11', '12'),\n",
    "                   'clf_alpha': [1, 1e-1, 1e-2]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc1c9e57-978b-4310-beab-386ff2933757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # Convert text to a matrix of token counts\n",
    "    ('tfidf', TfidfTransformer()),  # Transform counts to TF-IDF representation\n",
    "    ('clf', MultinomialNB())  # Apply Multinomial Naive Bayes for classification\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],  # N-gram range for tokenization\n",
    "    'tfidf__use_idf': [True, False],  # Whether to use IDF weighting\n",
    "    'tfidf__norm': ['l1', 'l2'],  # Normalization options\n",
    "    'clf__alpha': [1, 0.1, 0.01]  # Smoothing parameter for MultinomialNB\n",
    "}\n",
    "\n",
    "# Note:\n",
    "# - Double underscores (`__`) are used to access the parameters of pipeline components.\n",
    "# - Corrected parameter names to match proper syntax (`vect__ngram_range`, `tfidf__use_idf`, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcc076bf-bdd8-4c16-95ce-f8f140cebcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Parameters: {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "Best Cross-Validation Score: 0.44579823480017194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(text_clf, tuned_parameters, scoring='accuracy', cv=5, verbose=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Display the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac649-57fc-46e5-a477-b979a07d2f8f",
   "metadata": {},
   "source": [
    "## Task 5:-  Perform classification (using GridSearch) - 2 Marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5278f5a7-14d7-4f73-ab6e-ba41d721b17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.55      0.36      0.44      1572\n",
      "Extremely Positive       0.55      0.40      0.46      1989\n",
      "          Negative       0.41      0.48      0.44      3005\n",
      "           Neutral       0.65      0.39      0.48      2292\n",
      "          Positive       0.41      0.58      0.48      3490\n",
      "\n",
      "          accuracy                           0.46     12348\n",
      "         macro avg       0.51      0.44      0.46     12348\n",
      "      weighted avg       0.49      0.46      0.46     12348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "# Get the best model from GridSearchCV\n",
    "bestModel = grid_search.best_estimator_\n",
    "\n",
    "# Predict the sentiment on the test data\n",
    "y_pred = bestModel.predict(X_test)\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc825d-2586-47eb-a0e5-b9e783646d6b",
   "metadata": {},
   "source": [
    "## Task 6 :- Print the confusion matrix, accuracy, and F1 score on the test dataset - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37ac30b0-65d7-41c4-ab5c-eac5d9b678f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 566   21  709   36  240]\n",
      " [  20  791  169   49  960]\n",
      " [ 310  109 1437  176  973]\n",
      " [  56   85  486  887  778]\n",
      " [  71  445  726  220 2028]]\n",
      "Accuracy: 0.4623\n",
      "F1 Score (Weighted): 0.4620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Step 1: Predict on the test set\n",
    "y_pred = bestModel.predict(X_test)\n",
    "\n",
    "# Step 2: Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Step 3: Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 4: Compute F1 score (you can also specify 'weighted' if you have imbalanced classes)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score (Weighted): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e6046-2243-45f3-809a-5ef2f59d7e18",
   "metadata": {},
   "source": [
    "## Task 7:- Interpret your results in terms of Business Domain Knowledge. 1 Mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c65ba-2d4a-407d-8fd2-33dbb561f434",
   "metadata": {},
   "source": [
    "In the confusion matrix, the rows represent the true labels, and the columns represent the predicted labels.\n",
    "\n",
    "Key Observations:\n",
    "Class 0 (True vs. Predicted):\n",
    "\n",
    "There are 566 instances correctly classified as Class 0 (True Positives).\n",
    "709 instances were incorrectly classified as Class 2, 240 instances as Class 4, and only 36 instances as Class 3.\n",
    "A significant portion of the instances from Class 0 is being misclassified into Class 2.\n",
    "Class 1:\n",
    "\n",
    "791 instances correctly classified as Class 1 (True Positives).\n",
    "Misclassification into other classes: 169 into Class 2, 960 into Class 4.\n",
    "This suggests that the model struggles to distinguish Class 1 from Class 2 and Class 4.\n",
    "Class 2:\n",
    "\n",
    "1437 instances are correctly identified as Class 2 (True Positives).\n",
    "A fair number of misclassifications into Class 0 (310) and Class 1 (109), but still a relatively high number of true positives.\n",
    "Class 3:\n",
    "\n",
    "887 instances are correctly classified as Class 3 (True Positives).\n",
    "486 instances misclassified into Class 2 and 778 into Class 4, indicating significant confusion with these classes.\n",
    "Class 4:\n",
    "\n",
    "2028 instances correctly classified as Class 4 (True Positives).\n",
    "Misclassifications into Class 1 (445) and Class 2 (726) are notable, with 220 instances incorrectly classified as Class 3.\n",
    "Accuracy: 0.4623\n",
    "Interpretation: The model has an accuracy of 46.23%, meaning the model correctly predicted the class for about 46.23% of the instances. This is a relatively low accuracy, indicating that the model might be struggling to distinguish between certain classes, or the dataset might be highly imbalanced.\n",
    "Business Insight: If this is a classification task where accurate predictions are critical (e.g., fraud detection, customer segmentation), this low accuracy may not be sufficient for reliable decision-making. Further improvements in feature engineering, model tuning, or data preprocessing are needed.\n",
    "F1 Score (Weighted): 0.4620\n",
    "Interpretation: The F1 score (weighted) is 0.4620, which is close to the accuracy. The weighted F1 score accounts for class imbalance by giving higher weight to the classes with more instances. This score suggests that the model performs poorly across the classes, as the F1 score considers both precision and recall (balancing false positives and false negatives).\n",
    "Business Insight: This F1 score suggests that the model is not effective at identifying positive cases in many classes, and may have a high rate of both false positives and false negatives. Improving this score should be a priority if the model is to be used in business applications where missing a true positive or flagging too many false positives can have significant costs.\n",
    "Potential Business Implications and Actions:\n",
    "Class Imbalance:\n",
    "\n",
    "If the dataset is imbalanced (i.e., some classes have many more samples than others), this can cause the model to favor the majority class. Consider using class weighting or techniques like SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.\n",
    "Model Tuning:\n",
    "\n",
    "The model might not be tuned optimally. Consider running a GridSearchCV or RandomizedSearchCV to optimize hyperparameters and improve performance.\n",
    "Feature Engineering:\n",
    "\n",
    "Review the features being used to train the model. It might be necessary to extract more informative features, handle missing data, or remove irrelevant ones.\n",
    "Additional Metrics:\n",
    "\n",
    "Depending on the business use case, consider evaluating precision, recall, or ROC AUC scores for each class individually. This would provide more granular insights into the model's performance across different classes, especially in cases where false negatives or false positives are more costly.\n",
    "Model Choice:\n",
    "\n",
    "You may want to try different classifiers (e.g., Random Forest, XGBoost, SVM) to see if other models perform better than Naive Bayes in distinguishing between the classes.\n",
    "\n",
    "Next Steps:\n",
    "Data Cleaning and Preprocessing: Investigate potential issues with data quality, such as incorrect labels or noisy data.\n",
    "Improve Feature Selection: Consider using more domain-specific features or advanced techniques like TF-IDF or word embeddings (if it's text-based data).\n",
    "Hyperparameter Tuning: Fine-tune the model to improve its performance, using techniques like cross-validation to get more stable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
